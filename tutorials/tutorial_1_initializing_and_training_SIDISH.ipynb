{"cells":[{"cell_type":"markdown","metadata":{},"source":"# **Tutorial 1: Setting Up and Training SIDISH**\nThis tutorial explains how to initialize and train the SIDISH framework. SIDISH combines a Variational Autoencoder (VAE) and Deep Cox regression to uncover high-risk cell populations and predict clinical outcomes.\n### In this tutorial, you will:\n- Load the prepared data from Tutorial 0.  \n- Initialize SIDISH with appropriate model architecture, learning rates, and optimizer settings. \n### Outcome:\nBy the end of this tutorial, you will have a fully trained SIDISH model capable of identifying high-risk cell subpopulations and linking them to clinical outcomes.\n"},{"cell_type":"markdown","metadata":{},"source":"## **Step 1: Environment Setup**\n"},{"cell_type":"markdown","metadata":{},"source":"### **1.1 Set SIDISH conda environment**\nTo ensure compatibility, SIDISH requires Python 3.12. For best results, we recommend creating a virtual environment to manage dependencies:\n\nCreate a conda environment:\n```bash\nconda create --name sidish_env python=3.12\n```\nActivate the environment:\n```bash\nconda activate sidish_env\n```"},{"cell_type":"markdown","metadata":{},"source":"### **1.2 Install SIDISH**\nIn codeocean, there is no need to install SIDISH, since it would be already installed in the capsule. But to ensure that the SIDISH installed is the latest:\n- delete SIDISH from your pip libraries\n- add: git+https://github.com/mcgilldinglab/SIDISH.git#egg=sidish at the bottom of your bulk list in the pip install\n"},{"cell_type":"markdown","metadata":{},"source":"## **Step 2: Import libraries**\n### **2.1 Import SIDISH**\nThe SIDISH framework is imported directly for use:"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"from SIDISH import SIDISH as sidish"},{"cell_type":"markdown","metadata":{},"source":"### **2.2 Import Additional Libraries**\nAdditional libraries for data handling, visualization, and deep learning are required:"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"import scanpy as sc\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nimport os\nimport matplotlib.pyplot as plt"},{"cell_type":"markdown","metadata":{},"source":"### **2.3 Set Seeds**\nTo ensure results are consistent across multiple runs, set seeds for all key libraries:"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"seed = 0\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nnp.random.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nrandom.seed(1)\nite = 0\n# Set seeds for reproducibility\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Call the seed setting function\nset_seed(seed)"},{"cell_type":"markdown","metadata":{},"source":"## **Step 3: Reading Data sets**\nWe load the processed data prepared in Tutorial 0. Loading both the scRNA-seq and bulk RNA-seq data ensures that SIDISH has access to the required datasets for training."},{"cell_type":"markdown","metadata":{},"source":"### **3.1 Read single-cell data**\nSIDISH requires initialization before training. Each phase must be set up with appropriate parameters."},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"# Read single-cell RNA-seq data\nadata = sc.read_h5ad(\"../data/processed_adata.h5ad\")"},{"cell_type":"markdown","metadata":{},"source":"### **3.2 Read bulk and survival data**"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# Read bulk RNA-seq\nbulk = pd.read_csv(\"../data/processed_bulk.csv\", index_col=0)"},{"cell_type":"markdown","metadata":{},"source":"## **Step 4: Initializing SIDISH**\nInitialise SIDISH model with the saved single-cell and merged bulk RNA-seq data. This functionality also sets the device (\"cpu\" or \"cuda\") as well as the seed for reproductibility"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SIDISH No spatial graph used. Proceeding with dense VAE.\n"]}],"source":"sdh = sidish(adata, bulk, \"cuda\", seed=ite)"},{"cell_type":"markdown","metadata":{},"source":"### **4.1 Initialise Phase 1 of SIDISH**\nThis feature initialises the hyperparameters needed for Phase 1 in SIDISH. In Phase 1 of SIDISH, a Variational Autoencoder compresses the single-cell data into a biologically meaningful latent space to extract key cellular patterns. The `epoch` parameter sets the number of iterations to train the VAE i iteration 1 of SIDISH, whilst the `i_epoch` parameter sets the number of epochs to retrain the VAE after iteration 1 of SIDISH. `latent_size` determines the latent space size of the VAE, which we set to 32, and `layer_dims` determines the layer dimensions of the encoder and decoder of the VAE, in this example we set it to a two layer of size 512 and 128. Also `batch_size` determines the batch size of the single-cell data used to train the  VAE, which we set to 512 and `optimizer` determines the optimizer used to train the VAE, for the Lung dataset we used the Adam optimizer. `lr`and `lr_3` are the learning rate used to train the VAE in SIDISH at iteration 1 and after iteration 1 respectively. "},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"# Reduced batch size to avoid memory issues and ensure num_workers=0\nsdh.init_Phase1(225, 20, 32, [512, 128], 256, \"Adam\", 1.0e-4, 1e-4, 0)"},{"cell_type":"markdown","metadata":{},"source":"### **4.2 Initialise Phase 2 of SIDISH**\nThis feature initialises the hyperparameters needed for Phase 2 in SIDISH. In Phase 2 of SIDISH, a deep Cox regression model predicts patient survival risks using bulk RNA-seq profiles B and survival outcomes. Transfer learning reuses the encoder from the single-cell VAE, allowing FC to leverage high-resolution transcriptomic features while reducing redundancy in feature\ndiscovery. The `epoch` parameter sets the number of epochs to train the Deep Cox regression model.`hidden` parameter sets the number of additional fully connected layers to add to the encoder of the previously trained VAE, we used 128 in the case of the lung cancer dataset. `lr` determines the learning rate used to train the Deep Cox regression model. `test_size` is the size of the test set used to evaluate the performance of the deep Cox regression, used 20%. Also `batch_size` sets the batch size of the bulk data used to train the regressor, which we set to 256."},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"sdh.init_Phase2(500, 128, 1e-4, 0, 0.2, 256)"},{"cell_type":"markdown","metadata":{},"source":"## **Step 5: Start Training SIDISH**\nTo start training SIDISH, the number of iterations must be provided as well as the percentile threshold to define the number of High-Risk cells identified. In the case of the lung cancer dataset we set `iterations` and `percentile` to 5 and 0.95 respectively. It's important to note that the higher the `percentile` parameter is, the lower the number of cells will be considered as the High-Risk. The stepness of the sigmoid function used to generate the gene weights for the weight matrix, is determined by the `steepness` parameter which is set to 30. Finally, the output directory of the resulting files after training is provided. The folder will contain the annoted adata file containing which cell is considered as High-Risk or Background. It also contained the saved final deep Cox regression model as well as the VAE model. It also contained the gene weights matrix at each iteration."},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["########################################## Using Dense VAE ##########################################\n","########################################## ITERATION 1 OUT OF 5 ##########################################\n","[epoch 000]  average training loss: 1019.8280\n","[epoch 000]  average training loss: 1019.8280\n","[epoch 001]  average training loss: 919.7375\n","[epoch 001]  average training loss: 919.7375\n","[epoch 002]  average training loss: 883.8756\n","[epoch 002]  average training loss: 883.8756\n","[epoch 003]  average training loss: 870.8430\n","[epoch 003]  average training loss: 870.8430\n","[epoch 004]  average training loss: 865.9922\n","[epoch 004]  average training loss: 865.9922\n","[epoch 005]  average training loss: 864.0272\n","[epoch 005]  average training loss: 864.0272\n","[epoch 006]  average training loss: 863.0421\n","[epoch 006]  average training loss: 863.0421\n","[epoch 007]  average training loss: 862.0874\n","[epoch 007]  average training loss: 862.0874\n","[epoch 008]  average training loss: 860.5962\n","[epoch 008]  average training loss: 860.5962\n","[epoch 009]  average training loss: 856.8683\n","[epoch 009]  average training loss: 856.8683\n","[epoch 010]  average training loss: 844.6160\n","[epoch 010]  average training loss: 844.6160\n","[epoch 011]  average training loss: 809.9574\n","[epoch 011]  average training loss: 809.9574\n","[epoch 012]  average training loss: 769.4714\n","[epoch 012]  average training loss: 769.4714\n","[epoch 013]  average training loss: 750.1228\n","[epoch 013]  average training loss: 750.1228\n","[epoch 014]  average training loss: 741.0404\n","[epoch 014]  average training loss: 741.0404\n","[epoch 015]  average training loss: 736.2360\n","[epoch 015]  average training loss: 736.2360\n","[epoch 016]  average training loss: 733.3978\n","[epoch 016]  average training loss: 733.3978\n","[epoch 017]  average training loss: 731.0839\n","[epoch 017]  average training loss: 731.0839\n","[epoch 018]  average training loss: 729.7780\n","[epoch 018]  average training loss: 729.7780\n","[epoch 019]  average training loss: 728.5055\n","[epoch 019]  average training loss: 728.5055\n","[epoch 020]  average training loss: 727.6145\n","[epoch 020]  average training loss: 727.6145\n","[epoch 021]  average training loss: 727.1397\n","[epoch 021]  average training loss: 727.1397\n","[epoch 022]  average training loss: 726.6375\n","[epoch 022]  average training loss: 726.6375\n","[epoch 023]  average training loss: 726.2008\n","[epoch 023]  average training loss: 726.2008\n","[epoch 024]  average training loss: 725.7698\n","[epoch 024]  average training loss: 725.7698\n","[epoch 025]  average training loss: 725.4127\n","[epoch 025]  average training loss: 725.4127\n","[epoch 026]  average training loss: 725.3708\n","[epoch 026]  average training loss: 725.3708\n","[epoch 027]  average training loss: 725.0069\n","[epoch 027]  average training loss: 725.0069\n","[epoch 028]  average training loss: 724.9323\n","[epoch 028]  average training loss: 724.9323\n","[epoch 029]  average training loss: 724.7698\n","[epoch 029]  average training loss: 724.7698\n","[epoch 030]  average training loss: 724.6992\n","[epoch 030]  average training loss: 724.6992\n","[epoch 031]  average training loss: 723.8693\n","[epoch 031]  average training loss: 723.8693\n","[epoch 032]  average training loss: 723.7178\n","[epoch 032]  average training loss: 723.7178\n","[epoch 033]  average training loss: 723.4467\n","[epoch 033]  average training loss: 723.4467\n","[epoch 034]  average training loss: 723.3761\n","[epoch 034]  average training loss: 723.3761\n","[epoch 035]  average training loss: 722.8692\n","[epoch 035]  average training loss: 722.8692\n","[epoch 036]  average training loss: 722.2480\n","[epoch 036]  average training loss: 722.2480\n","[epoch 037]  average training loss: 721.4743\n","[epoch 037]  average training loss: 721.4743\n","[epoch 038]  average training loss: 720.1836\n","[epoch 038]  average training loss: 720.1836\n","[epoch 039]  average training loss: 718.3428\n","[epoch 039]  average training loss: 718.3428\n","[epoch 040]  average training loss: 715.5470\n","[epoch 040]  average training loss: 715.5470\n","[epoch 041]  average training loss: 712.3423\n","[epoch 041]  average training loss: 712.3423\n","[epoch 042]  average training loss: 709.3348\n","[epoch 042]  average training loss: 709.3348\n","[epoch 043]  average training loss: 707.3828\n","[epoch 043]  average training loss: 707.3828\n","[epoch 044]  average training loss: 705.6982\n","[epoch 044]  average training loss: 705.6982\n","[epoch 045]  average training loss: 703.9218\n","[epoch 045]  average training loss: 703.9218\n","[epoch 046]  average training loss: 702.5810\n","[epoch 046]  average training loss: 702.5810\n","[epoch 047]  average training loss: 701.7023\n","[epoch 047]  average training loss: 701.7023\n","[epoch 048]  average training loss: 700.7300\n","[epoch 048]  average training loss: 700.7300\n","[epoch 049]  average training loss: 699.6256\n","[epoch 049]  average training loss: 699.6256\n","[epoch 050]  average training loss: 698.8238\n","[epoch 050]  average training loss: 698.8238\n","[epoch 051]  average training loss: 698.3805\n","[epoch 051]  average training loss: 698.3805\n","[epoch 052]  average training loss: 697.3993\n","[epoch 052]  average training loss: 697.3993\n","[epoch 053]  average training loss: 697.3876\n","[epoch 053]  average training loss: 697.3876\n","[epoch 054]  average training loss: 696.5604\n","[epoch 054]  average training loss: 696.5604\n","[epoch 055]  average training loss: 696.2330\n","[epoch 055]  average training loss: 696.2330\n","[epoch 056]  average training loss: 695.9444\n","[epoch 056]  average training loss: 695.9444\n","[epoch 057]  average training loss: 695.7446\n","[epoch 057]  average training loss: 695.7446\n","[epoch 058]  average training loss: 695.5883\n","[epoch 058]  average training loss: 695.5883\n","[epoch 059]  average training loss: 695.4729\n","[epoch 059]  average training loss: 695.4729\n","[epoch 060]  average training loss: 694.9378\n","[epoch 060]  average training loss: 694.9378\n","[epoch 061]  average training loss: 694.9171\n","[epoch 061]  average training loss: 694.9171\n","[epoch 062]  average training loss: 694.7635\n","[epoch 062]  average training loss: 694.7635\n","[epoch 063]  average training loss: 694.5001\n","[epoch 063]  average training loss: 694.5001\n","[epoch 064]  average training loss: 694.3007\n","[epoch 064]  average training loss: 694.3007\n","[epoch 065]  average training loss: 694.2043\n","[epoch 065]  average training loss: 694.2043\n","[epoch 066]  average training loss: 693.9591\n","[epoch 066]  average training loss: 693.9591\n","[epoch 067]  average training loss: 693.6080\n","[epoch 067]  average training loss: 693.6080\n","[epoch 068]  average training loss: 693.3442\n","[epoch 068]  average training loss: 693.3442\n","[epoch 069]  average training loss: 693.3396\n","[epoch 069]  average training loss: 693.3396\n","[epoch 070]  average training loss: 693.4688\n","[epoch 070]  average training loss: 693.4688\n","[epoch 071]  average training loss: 693.2494\n","[epoch 071]  average training loss: 693.2494\n","[epoch 072]  average training loss: 693.0467\n","[epoch 072]  average training loss: 693.0467\n","[epoch 073]  average training loss: 693.0148\n","[epoch 073]  average training loss: 693.0148\n","[epoch 074]  average training loss: 692.8182\n","[epoch 074]  average training loss: 692.8182\n","[epoch 075]  average training loss: 692.4944\n","[epoch 075]  average training loss: 692.4944\n","[epoch 076]  average training loss: 692.4528\n","[epoch 076]  average training loss: 692.4528\n","[epoch 077]  average training loss: 692.0883\n","[epoch 077]  average training loss: 692.0883\n","[epoch 078]  average training loss: 692.0854\n","[epoch 078]  average training loss: 692.0854\n","[epoch 079]  average training loss: 691.7688\n","[epoch 079]  average training loss: 691.7688\n","[epoch 080]  average training loss: 691.8194\n","[epoch 080]  average training loss: 691.8194\n","[epoch 081]  average training loss: 691.7176\n","[epoch 081]  average training loss: 691.7176\n","[epoch 082]  average training loss: 691.5398\n","[epoch 082]  average training loss: 691.5398\n","[epoch 083]  average training loss: 691.3032\n","[epoch 083]  average training loss: 691.3032\n","[epoch 084]  average training loss: 691.2185\n","[epoch 084]  average training loss: 691.2185\n","[epoch 085]  average training loss: 690.9488\n","[epoch 085]  average training loss: 690.9488\n","[epoch 086]  average training loss: 690.9359\n","[epoch 086]  average training loss: 690.9359\n","[epoch 087]  average training loss: 690.9021\n","[epoch 087]  average training loss: 690.9021\n","[epoch 088]  average training loss: 690.6143\n","[epoch 088]  average training loss: 690.6143\n","[epoch 089]  average training loss: 690.6182\n","[epoch 089]  average training loss: 690.6182\n","[epoch 090]  average training loss: 690.3900\n","[epoch 090]  average training loss: 690.3900\n","[epoch 091]  average training loss: 690.2143\n","[epoch 091]  average training loss: 690.2143\n","[epoch 092]  average training loss: 690.1578\n","[epoch 092]  average training loss: 690.1578\n","[epoch 093]  average training loss: 689.9750\n","[epoch 093]  average training loss: 689.9750\n","[epoch 094]  average training loss: 690.0071\n","[epoch 094]  average training loss: 690.0071\n","[epoch 095]  average training loss: 689.6158\n","[epoch 095]  average training loss: 689.6158\n","[epoch 096]  average training loss: 689.7002\n","[epoch 096]  average training loss: 689.7002\n","[epoch 097]  average training loss: 689.3715\n","[epoch 097]  average training loss: 689.3715\n","[epoch 098]  average training loss: 689.5448\n","[epoch 098]  average training loss: 689.5448\n","[epoch 099]  average training loss: 689.1733\n","[epoch 099]  average training loss: 689.1733\n","[epoch 100]  average training loss: 689.0630\n","[epoch 100]  average training loss: 689.0630\n","[epoch 101]  average training loss: 688.8658\n","[epoch 101]  average training loss: 688.8658\n","[epoch 102]  average training loss: 688.8252\n","[epoch 102]  average training loss: 688.8252\n","[epoch 103]  average training loss: 688.7210\n","[epoch 103]  average training loss: 688.7210\n","[epoch 104]  average training loss: 688.6150\n","[epoch 104]  average training loss: 688.6150\n","[epoch 105]  average training loss: 688.4387\n","[epoch 105]  average training loss: 688.4387\n","[epoch 106]  average training loss: 688.2709\n","[epoch 106]  average training loss: 688.2709\n","[epoch 107]  average training loss: 688.2361\n","[epoch 107]  average training loss: 688.2361\n","[epoch 108]  average training loss: 687.9005\n","[epoch 108]  average training loss: 687.9005\n","[epoch 109]  average training loss: 687.8142\n","[epoch 109]  average training loss: 687.8142\n","[epoch 110]  average training loss: 687.6280\n","[epoch 110]  average training loss: 687.6280\n","[epoch 111]  average training loss: 687.5828\n","[epoch 111]  average training loss: 687.5828\n","[epoch 112]  average training loss: 687.4989\n","[epoch 112]  average training loss: 687.4989\n","[epoch 113]  average training loss: 687.4621\n","[epoch 113]  average training loss: 687.4621\n","[epoch 114]  average training loss: 687.4730\n","[epoch 114]  average training loss: 687.4730\n","[epoch 115]  average training loss: 687.0757\n","[epoch 115]  average training loss: 687.0757\n","[epoch 116]  average training loss: 687.0762\n","[epoch 116]  average training loss: 687.0762\n","[epoch 117]  average training loss: 686.8273\n","[epoch 117]  average training loss: 686.8273\n","[epoch 118]  average training loss: 686.6438\n","[epoch 118]  average training loss: 686.6438\n","[epoch 119]  average training loss: 686.7323\n","[epoch 119]  average training loss: 686.7323\n","[epoch 120]  average training loss: 686.5858\n","[epoch 120]  average training loss: 686.5858\n","[epoch 121]  average training loss: 686.3986\n","[epoch 121]  average training loss: 686.3986\n","[epoch 122]  average training loss: 686.3152\n","[epoch 122]  average training loss: 686.3152\n","[epoch 123]  average training loss: 686.2547\n","[epoch 123]  average training loss: 686.2547\n","[epoch 124]  average training loss: 686.2124\n","[epoch 124]  average training loss: 686.2124\n","[epoch 125]  average training loss: 686.2129\n","[epoch 125]  average training loss: 686.2129\n","[epoch 126]  average training loss: 685.9548\n","[epoch 126]  average training loss: 685.9548\n","[epoch 127]  average training loss: 685.6740\n","[epoch 127]  average training loss: 685.6740\n","[epoch 128]  average training loss: 685.5931\n","[epoch 128]  average training loss: 685.5931\n","[epoch 129]  average training loss: 685.5929\n","[epoch 129]  average training loss: 685.5929\n","[epoch 130]  average training loss: 685.4006\n","[epoch 130]  average training loss: 685.4006\n","[epoch 131]  average training loss: 685.4268\n","[epoch 131]  average training loss: 685.4268\n","[epoch 132]  average training loss: 685.2497\n","[epoch 132]  average training loss: 685.2497\n","[epoch 133]  average training loss: 685.2318\n","[epoch 133]  average training loss: 685.2318\n","[epoch 134]  average training loss: 684.9702\n","[epoch 134]  average training loss: 684.9702\n","[epoch 135]  average training loss: 684.9981\n","[epoch 135]  average training loss: 684.9981\n","[epoch 136]  average training loss: 684.8778\n","[epoch 136]  average training loss: 684.8778\n","[epoch 137]  average training loss: 684.6570\n","[epoch 137]  average training loss: 684.6570\n","[epoch 138]  average training loss: 684.3543\n","[epoch 138]  average training loss: 684.3543\n","[epoch 139]  average training loss: 684.4128\n","[epoch 139]  average training loss: 684.4128\n","[epoch 140]  average training loss: 684.4486\n","[epoch 140]  average training loss: 684.4486\n","[epoch 141]  average training loss: 684.1872\n","[epoch 141]  average training loss: 684.1872\n","[epoch 142]  average training loss: 684.0749\n","[epoch 142]  average training loss: 684.0749\n","[epoch 143]  average training loss: 683.8388\n","[epoch 143]  average training loss: 683.8388\n","[epoch 144]  average training loss: 683.7671\n","[epoch 144]  average training loss: 683.7671\n","[epoch 145]  average training loss: 683.6705\n","[epoch 145]  average training loss: 683.6705\n","[epoch 146]  average training loss: 683.6645\n","[epoch 146]  average training loss: 683.6645\n","[epoch 147]  average training loss: 683.4125\n","[epoch 147]  average training loss: 683.4125\n","[epoch 148]  average training loss: 683.2440\n","[epoch 148]  average training loss: 683.2440\n","[epoch 149]  average training loss: 683.2303\n","[epoch 149]  average training loss: 683.2303\n","[epoch 150]  average training loss: 683.1450\n","[epoch 150]  average training loss: 683.1450\n","[epoch 151]  average training loss: 682.8960\n","[epoch 151]  average training loss: 682.8960\n","[epoch 152]  average training loss: 682.8979\n","[epoch 152]  average training loss: 682.8979\n","[epoch 153]  average training loss: 682.6638\n","[epoch 153]  average training loss: 682.6638\n","[epoch 154]  average training loss: 682.6380\n","[epoch 154]  average training loss: 682.6380\n","[epoch 155]  average training loss: 682.5202\n","[epoch 155]  average training loss: 682.5202\n","[epoch 156]  average training loss: 682.3899\n","[epoch 156]  average training loss: 682.3899\n","[epoch 157]  average training loss: 682.3220\n","[epoch 157]  average training loss: 682.3220\n","[epoch 158]  average training loss: 682.1077\n","[epoch 158]  average training loss: 682.1077\n","[epoch 159]  average training loss: 682.1785\n","[epoch 159]  average training loss: 682.1785\n","[epoch 160]  average training loss: 682.0943\n","[epoch 160]  average training loss: 682.0943\n","[epoch 161]  average training loss: 681.8620\n","[epoch 161]  average training loss: 681.8620\n","[epoch 162]  average training loss: 681.8488\n","[epoch 162]  average training loss: 681.8488\n","[epoch 163]  average training loss: 681.6464\n","[epoch 163]  average training loss: 681.6464\n","[epoch 164]  average training loss: 681.5616\n","[epoch 164]  average training loss: 681.5616\n","[epoch 165]  average training loss: 681.3032\n","[epoch 165]  average training loss: 681.3032\n","[epoch 166]  average training loss: 681.1611\n","[epoch 166]  average training loss: 681.1611\n","[epoch 167]  average training loss: 681.1744\n","[epoch 167]  average training loss: 681.1744\n","[epoch 168]  average training loss: 681.0602\n","[epoch 168]  average training loss: 681.0602\n","[epoch 169]  average training loss: 680.9622\n","[epoch 169]  average training loss: 680.9622\n","[epoch 170]  average training loss: 680.8719\n","[epoch 170]  average training loss: 680.8719\n","[epoch 171]  average training loss: 680.6923\n","[epoch 171]  average training loss: 680.6923\n","[epoch 172]  average training loss: 680.6429\n","[epoch 172]  average training loss: 680.6429\n","[epoch 173]  average training loss: 680.4657\n","[epoch 173]  average training loss: 680.4657\n","[epoch 174]  average training loss: 680.4087\n","[epoch 174]  average training loss: 680.4087\n","[epoch 175]  average training loss: 680.3195\n","[epoch 175]  average training loss: 680.3195\n","[epoch 176]  average training loss: 680.1623\n","[epoch 176]  average training loss: 680.1623\n","[epoch 177]  average training loss: 679.9472\n","[epoch 177]  average training loss: 679.9472\n","[epoch 178]  average training loss: 679.9777\n","[epoch 178]  average training loss: 679.9777\n","[epoch 179]  average training loss: 679.6911\n","[epoch 179]  average training loss: 679.6911\n","[epoch 180]  average training loss: 679.5475\n","[epoch 180]  average training loss: 679.5475\n","[epoch 181]  average training loss: 679.3507\n","[epoch 181]  average training loss: 679.3507\n","[epoch 182]  average training loss: 679.2551\n","[epoch 182]  average training loss: 679.2551\n","[epoch 183]  average training loss: 679.2054\n","[epoch 183]  average training loss: 679.2054\n","[epoch 184]  average training loss: 678.9152\n","[epoch 184]  average training loss: 678.9152\n","[epoch 185]  average training loss: 678.7311\n","[epoch 185]  average training loss: 678.7311\n","[epoch 186]  average training loss: 678.6211\n","[epoch 186]  average training loss: 678.6211\n","[epoch 187]  average training loss: 678.4286\n","[epoch 187]  average training loss: 678.4286\n","[epoch 188]  average training loss: 678.1121\n","[epoch 188]  average training loss: 678.1121\n","[epoch 189]  average training loss: 677.9365\n","[epoch 189]  average training loss: 677.9365\n","[epoch 190]  average training loss: 677.7372\n","[epoch 190]  average training loss: 677.7372\n","[epoch 191]  average training loss: 677.5794\n","[epoch 191]  average training loss: 677.5794\n","[epoch 192]  average training loss: 677.3200\n","[epoch 192]  average training loss: 677.3200\n","[epoch 193]  average training loss: 677.2754\n","[epoch 193]  average training loss: 677.2754\n","[epoch 194]  average training loss: 677.1419\n","[epoch 194]  average training loss: 677.1419\n","[epoch 195]  average training loss: 676.7313\n","[epoch 195]  average training loss: 676.7313\n","[epoch 196]  average training loss: 676.6336\n","[epoch 196]  average training loss: 676.6336\n","[epoch 197]  average training loss: 676.2787\n","[epoch 197]  average training loss: 676.2787\n","[epoch 198]  average training loss: 676.2978\n","[epoch 198]  average training loss: 676.2978\n","[epoch 199]  average training loss: 675.9176\n","[epoch 199]  average training loss: 675.9176\n","[epoch 200]  average training loss: 675.9114\n","[epoch 200]  average training loss: 675.9114\n","[epoch 201]  average training loss: 675.6629\n","[epoch 201]  average training loss: 675.6629\n","[epoch 202]  average training loss: 675.6840\n","[epoch 202]  average training loss: 675.6840\n","[epoch 203]  average training loss: 675.4436\n","[epoch 203]  average training loss: 675.4436\n","[epoch 204]  average training loss: 675.2646\n","[epoch 204]  average training loss: 675.2646\n","[epoch 205]  average training loss: 675.1021\n","[epoch 205]  average training loss: 675.1021\n","[epoch 206]  average training loss: 674.9813\n","[epoch 206]  average training loss: 674.9813\n","[epoch 207]  average training loss: 674.8181\n","[epoch 207]  average training loss: 674.8181\n","[epoch 208]  average training loss: 674.6617\n","[epoch 208]  average training loss: 674.6617\n","[epoch 209]  average training loss: 674.6330\n","[epoch 209]  average training loss: 674.6330\n","[epoch 210]  average training loss: 674.5291\n","[epoch 210]  average training loss: 674.5291\n","[epoch 211]  average training loss: 674.2552\n","[epoch 211]  average training loss: 674.2552\n","[epoch 212]  average training loss: 674.0425\n","[epoch 212]  average training loss: 674.0425\n","[epoch 213]  average training loss: 674.0246\n","[epoch 213]  average training loss: 674.0246\n","[epoch 214]  average training loss: 673.7635\n","[epoch 214]  average training loss: 673.7635\n","[epoch 215]  average training loss: 673.7074\n","[epoch 215]  average training loss: 673.7074\n","[epoch 216]  average training loss: 673.6119\n","[epoch 216]  average training loss: 673.6119\n","[epoch 217]  average training loss: 673.5171\n","[epoch 217]  average training loss: 673.5171\n","[epoch 218]  average training loss: 673.2784\n","[epoch 218]  average training loss: 673.2784\n","[epoch 219]  average training loss: 673.1180\n","[epoch 219]  average training loss: 673.1180\n","[epoch 220]  average training loss: 673.1484\n","[epoch 220]  average training loss: 673.1484\n","[epoch 221]  average training loss: 673.0268\n","[epoch 221]  average training loss: 673.0268\n","[epoch 222]  average training loss: 672.8995\n","[epoch 222]  average training loss: 672.8995\n","[epoch 223]  average training loss: 672.7230\n","[epoch 223]  average training loss: 672.7230\n","[epoch 224]  average training loss: 672.8352\n","[epoch 224]  average training loss: 672.8352\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:04<00:00, 107.11it/s]\n","\n"]},{"name":"stdout","output_type":"stream","text":["########################################## Calculating Patients Weight Vector ##########################################\n","Gamma\n","Best Distribution: \n","########################################## Calculating Cells Weight Matrix ##########################################\n","########################################## Saving Weight Matrix at Iteration 0 ##########################################\n","########################################## Saving Weight Matrix at Iteration 0 ##########################################\n","########################################## ITERATION 2 OUT OF 5 ##########################################\n","########################################## ITERATION 2 OUT OF 5 ##########################################\n","[epoch 000]  average training loss: 701.0020\n","[epoch 000]  average training loss: 701.0020\n","[epoch 001]  average training loss: 700.5368\n","[epoch 001]  average training loss: 700.5368\n","[epoch 002]  average training loss: 700.0226\n","[epoch 002]  average training loss: 700.0226\n","[epoch 003]  average training loss: 699.9391\n","[epoch 003]  average training loss: 699.9391\n","[epoch 004]  average training loss: 700.8932\n","[epoch 004]  average training loss: 700.8932\n","[epoch 005]  average training loss: 701.7575\n","[epoch 005]  average training loss: 701.7575\n","[epoch 006]  average training loss: 700.8430\n","[epoch 007]  average training loss: 700.1986\n","[epoch 008]  average training loss: 699.5146\n","[epoch 009]  average training loss: 699.4839\n","[epoch 010]  average training loss: 699.0983\n","[epoch 011]  average training loss: 698.9924\n","[epoch 012]  average training loss: 699.0212\n","[epoch 013]  average training loss: 698.7250\n","[epoch 014]  average training loss: 698.6690\n","[epoch 015]  average training loss: 698.5483\n","[epoch 016]  average training loss: 698.4035\n","[epoch 017]  average training loss: 698.2526\n","[epoch 018]  average training loss: 698.2120\n","[epoch 019]  average training loss: 698.0271\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:04<00:00, 111.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["########################################## Calculating Patients Weight Vector ##########################################\n","########################################## Calculating Cells Weight Matrix ##########################################\n","########################################## Saving Weight Matrix at Iteration 1 ##########################################\n","########################################## ITERATION 3 OUT OF 5 ##########################################\n","[epoch 000]  average training loss: 756.8352\n","[epoch 001]  average training loss: 756.3136\n","[epoch 002]  average training loss: 755.7399\n","[epoch 003]  average training loss: 755.7021\n","[epoch 004]  average training loss: 755.4863\n","[epoch 005]  average training loss: 755.3154\n","[epoch 006]  average training loss: 755.2035\n","[epoch 007]  average training loss: 755.1225\n","[epoch 008]  average training loss: 754.8130\n","[epoch 009]  average training loss: 754.7450\n","[epoch 010]  average training loss: 754.5552\n","[epoch 011]  average training loss: 754.4223\n","[epoch 012]  average training loss: 754.3777\n","[epoch 013]  average training loss: 754.1250\n","[epoch 014]  average training loss: 753.9994\n","[epoch 015]  average training loss: 753.8288\n","[epoch 016]  average training loss: 753.7237\n","[epoch 017]  average training loss: 753.5800\n","[epoch 018]  average training loss: 753.5489\n","[epoch 019]  average training loss: 753.3719\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:04<00:00, 105.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["########################################## Calculating Patients Weight Vector ##########################################\n","########################################## Calculating Cells Weight Matrix ##########################################\n"]},{"name":"stderr","output_type":"stream","text":[" 98%|===================| 1614/1642 [00:12<00:00]        "]},{"name":"stdout","output_type":"stream","text":["########################################## Saving Weight Matrix at Iteration 2 ##########################################\n","########################################## ITERATION 4 OUT OF 5 ##########################################\n","[epoch 000]  average training loss: 789.0911\n","[epoch 001]  average training loss: 788.4635\n","[epoch 002]  average training loss: 787.9519\n","[epoch 003]  average training loss: 787.8799\n","[epoch 004]  average training loss: 787.7612\n","[epoch 005]  average training loss: 787.6008\n","[epoch 006]  average training loss: 787.5745\n","[epoch 007]  average training loss: 787.4798\n","[epoch 008]  average training loss: 787.1837\n","[epoch 009]  average training loss: 787.1570\n","[epoch 010]  average training loss: 787.0674\n","[epoch 011]  average training loss: 786.9060\n","[epoch 012]  average training loss: 786.9176\n","[epoch 013]  average training loss: 786.7096\n","[epoch 014]  average training loss: 786.5987\n","[epoch 015]  average training loss: 786.4724\n","[epoch 016]  average training loss: 786.3992\n","[epoch 017]  average training loss: 786.2706\n","[epoch 018]  average training loss: 786.2962\n","[epoch 019]  average training loss: 786.1610\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:04<00:00, 104.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["########################################## Calculating Patients Weight Vector ##########################################\n","########################################## Calculating Cells Weight Matrix ##########################################\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|=================== | 1568/1642 [00:11<00:00]       "]},{"name":"stdout","output_type":"stream","text":["########################################## Saving Weight Matrix at Iteration 3 ##########################################\n","########################################## ITERATION 5 OUT OF 5 ##########################################\n","[epoch 000]  average training loss: 791.1621\n","[epoch 001]  average training loss: 790.6148\n","[epoch 002]  average training loss: 790.1531\n","[epoch 003]  average training loss: 790.0888\n","[epoch 004]  average training loss: 789.9973\n","[epoch 005]  average training loss: 789.8375\n","[epoch 006]  average training loss: 789.8441\n","[epoch 007]  average training loss: 789.7551\n","[epoch 008]  average training loss: 789.4952\n","[epoch 009]  average training loss: 789.4782\n","[epoch 010]  average training loss: 789.4271\n","[epoch 011]  average training loss: 789.2819\n","[epoch 012]  average training loss: 789.3175\n","[epoch 013]  average training loss: 789.1155\n","[epoch 014]  average training loss: 789.0182\n","[epoch 015]  average training loss: 788.8968\n","[epoch 016]  average training loss: 788.8596\n","[epoch 017]  average training loss: 788.7283\n","[epoch 018]  average training loss: 788.7458\n","[epoch 019]  average training loss: 788.6572\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:04<00:00, 113.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["########################################## Calculating Patients Weight Vector ##########################################\n","########################################## Calculating Cells Weight Matrix ##########################################\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|=================== | 1538/1642 [00:11<00:00]       "]},{"name":"stdout","output_type":"stream","text":["########################################## Saving Weight Matrix at Iteration 4 ##########################################\n","########################################## SIDISH TRAINING DONE ##########################################\n","########################################## Saving Final AnnData Object ##########################################\n"]}],"source":"train_adata = sdh.train(5, 0.95, 30, \"../data/LUNG/\", distribution_fit='fitted')"}],"metadata":{"kernelspec":{"display_name":"test","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":4}