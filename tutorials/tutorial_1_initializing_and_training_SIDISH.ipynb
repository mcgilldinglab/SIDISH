{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial 1: Setting Up and Training SIDISH**\n",
    "This tutorial explains how to initialize and train the SIDISH framework. SIDISH combines a Variational Autoencoder (VAE) and Deep Cox regression to uncover high-risk cell populations and predict clinical outcomes.\n",
    "### In this tutorial, you will:\n",
    "- Load the prepared data from Tutorial 0.  \n",
    "- Initialize SIDISH with appropriate model architecture, learning rates, and optimizer settings. \n",
    "### Outcome:\n",
    "By the end of this tutorial, you will have a fully trained SIDISH model capable of identifying high-risk cell subpopulations and linking them to clinical outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Environment Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Install Required Libraries**\n",
    "To ensure compatibility, SIDISH requires Python 3.9 or higher. For best results, we recommend creating a virtual environment to manage dependencies:\n",
    "\n",
    "Create a conda environment:\n",
    "```bash\n",
    "conda create --name sidish_env\n",
    "```\n",
    "Activate the environment:\n",
    "```bash\n",
    "conda activate sidish_env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Install SIDISH**\n",
    "Once the environment is active, install dependencies using:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Then to install SIDISH, run:\n",
    "\n",
    "```bash\n",
    "pip install SIDISH==1.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Import libraries**\n",
    "### **2.1 Import SIDISH**\n",
    "The SIDISH framework is imported directly for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SIDISH import SIDISH as sidish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Import Additional Libraries**\n",
    "Additional libraries for data handling, visualization, and deep learning are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  torch\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Set Seeds**\n",
    "To ensure results are consistent across multiple runs, set seeds for all key libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(1)\n",
    "ite = 0\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call the seed setting function\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Reading Data sets**\n",
    "We load the processed data prepared in Tutorial 0. Loading both the scRNA-seq and bulk RNA-seq data ensures that SIDISH has access to the required datasets for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Read single-cell data**\n",
    "SIDISH requires initialization before training. Each phase must be set up with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read single-cell RNA-seq data\n",
    "adata = sc.read_h5ad(\"../../DATA/processed_adata.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Read bulk and survival data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read bulk RNA-seq\n",
    "bulk = pd.read_csv(\"../../DATA/processed_bulk.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Initializing SIDISH**\n",
    "Initialise SIDISH model with the saved single-cell and merged bulk RNA-seq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdh = sidish(adata, bulk, \"cpu\", seed=ite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Initialise Phase 1 of SIDISH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdh.init_Phase1(225, 20, 32, [512, 128], 512, \"Adam\", 1.0e-4, 1e-4, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Initialise Phase 2 of SIDISH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdh.init_Phase2(500, 128, 1e-4, 0, 0.2, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5: Start Training SIDISH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################## ITERATION 1 OUT OF 5 ##########################################\n",
      "[epoch 000]  average training loss: 1018.3363\n",
      "[epoch 001]  average training loss: 918.3922\n",
      "[epoch 002]  average training loss: 882.5828\n",
      "[epoch 003]  average training loss: 869.5693\n",
      "[epoch 004]  average training loss: 864.7255\n",
      "[epoch 005]  average training loss: 862.7633\n",
      "[epoch 006]  average training loss: 861.7798\n",
      "[epoch 007]  average training loss: 860.8265\n",
      "[epoch 008]  average training loss: 859.3374\n",
      "[epoch 009]  average training loss: 855.6150\n",
      "[epoch 010]  average training loss: 843.3806\n",
      "[epoch 011]  average training loss: 808.7727\n",
      "[epoch 012]  average training loss: 768.3458\n",
      "[epoch 013]  average training loss: 749.0256\n",
      "[epoch 014]  average training loss: 739.9565\n",
      "[epoch 015]  average training loss: 735.1591\n",
      "[epoch 016]  average training loss: 732.3251\n",
      "[epoch 017]  average training loss: 730.0145\n",
      "[epoch 018]  average training loss: 728.7105\n",
      "[epoch 019]  average training loss: 727.4399\n",
      "[epoch 020]  average training loss: 726.5503\n",
      "[epoch 021]  average training loss: 726.0761\n",
      "[epoch 022]  average training loss: 725.5746\n",
      "[epoch 023]  average training loss: 725.1386\n",
      "[epoch 024]  average training loss: 724.7082\n",
      "[epoch 025]  average training loss: 724.3516\n",
      "[epoch 026]  average training loss: 724.3098\n",
      "[epoch 027]  average training loss: 723.9464\n",
      "[epoch 028]  average training loss: 723.8719\n",
      "[epoch 029]  average training loss: 723.7097\n",
      "[epoch 030]  average training loss: 723.6392\n",
      "[epoch 031]  average training loss: 722.8104\n",
      "[epoch 032]  average training loss: 722.6592\n",
      "[epoch 033]  average training loss: 722.3885\n",
      "[epoch 034]  average training loss: 722.3180\n",
      "[epoch 035]  average training loss: 721.8118\n",
      "[epoch 036]  average training loss: 721.1916\n",
      "[epoch 037]  average training loss: 720.4190\n",
      "[epoch 038]  average training loss: 719.1302\n",
      "[epoch 039]  average training loss: 717.2921\n",
      "[epoch 040]  average training loss: 714.5004\n",
      "[epoch 041]  average training loss: 711.3004\n",
      "[epoch 042]  average training loss: 708.2973\n",
      "[epoch 043]  average training loss: 706.3481\n",
      "[epoch 044]  average training loss: 704.6659\n",
      "[epoch 045]  average training loss: 702.8922\n",
      "[epoch 046]  average training loss: 701.5534\n",
      "[epoch 047]  average training loss: 700.6759\n",
      "[epoch 048]  average training loss: 699.7050\n",
      "[epoch 049]  average training loss: 698.6023\n",
      "[epoch 050]  average training loss: 697.8016\n",
      "[epoch 051]  average training loss: 697.3590\n",
      "[epoch 052]  average training loss: 696.3792\n",
      "[epoch 053]  average training loss: 696.3675\n",
      "[epoch 054]  average training loss: 695.5416\n",
      "[epoch 055]  average training loss: 695.2147\n",
      "[epoch 056]  average training loss: 694.9264\n",
      "[epoch 057]  average training loss: 694.7270\n",
      "[epoch 058]  average training loss: 694.5709\n",
      "[epoch 059]  average training loss: 694.4556\n",
      "[epoch 060]  average training loss: 693.9213\n",
      "[epoch 061]  average training loss: 693.9007\n",
      "[epoch 062]  average training loss: 693.7473\n",
      "[epoch 063]  average training loss: 693.4842\n",
      "[epoch 064]  average training loss: 693.2851\n",
      "[epoch 065]  average training loss: 693.1889\n",
      "[epoch 066]  average training loss: 692.9440\n",
      "[epoch 067]  average training loss: 692.5934\n",
      "[epoch 068]  average training loss: 692.3300\n",
      "[epoch 069]  average training loss: 692.3255\n",
      "[epoch 070]  average training loss: 692.4545\n",
      "[epoch 071]  average training loss: 692.2354\n",
      "[epoch 072]  average training loss: 692.0329\n",
      "[epoch 073]  average training loss: 692.0011\n",
      "[epoch 074]  average training loss: 691.8048\n",
      "[epoch 075]  average training loss: 691.4815\n",
      "[epoch 076]  average training loss: 691.4399\n",
      "[epoch 077]  average training loss: 691.0760\n",
      "[epoch 078]  average training loss: 691.0731\n",
      "[epoch 079]  average training loss: 690.7570\n",
      "[epoch 080]  average training loss: 690.8074\n",
      "[epoch 081]  average training loss: 690.7058\n",
      "[epoch 082]  average training loss: 690.5283\n",
      "[epoch 083]  average training loss: 690.2920\n",
      "[epoch 084]  average training loss: 690.2075\n",
      "[epoch 085]  average training loss: 689.9382\n",
      "[epoch 086]  average training loss: 689.9253\n",
      "[epoch 087]  average training loss: 689.8915\n",
      "[epoch 088]  average training loss: 689.6042\n",
      "[epoch 089]  average training loss: 689.6081\n",
      "[epoch 090]  average training loss: 689.3802\n",
      "[epoch 091]  average training loss: 689.2047\n",
      "[epoch 092]  average training loss: 689.1483\n",
      "[epoch 093]  average training loss: 688.9658\n",
      "[epoch 094]  average training loss: 688.9978\n",
      "[epoch 095]  average training loss: 688.6071\n",
      "[epoch 096]  average training loss: 688.6914\n",
      "[epoch 097]  average training loss: 688.3632\n",
      "[epoch 098]  average training loss: 688.5362\n",
      "[epoch 099]  average training loss: 688.1652\n",
      "[epoch 100]  average training loss: 688.0551\n",
      "[epoch 101]  average training loss: 687.8582\n",
      "[epoch 102]  average training loss: 687.8177\n",
      "[epoch 103]  average training loss: 687.7136\n",
      "[epoch 104]  average training loss: 687.6078\n",
      "[epoch 105]  average training loss: 687.4318\n",
      "[epoch 106]  average training loss: 687.2642\n",
      "[epoch 107]  average training loss: 687.2294\n",
      "[epoch 108]  average training loss: 686.8943\n",
      "[epoch 109]  average training loss: 686.8081\n",
      "[epoch 110]  average training loss: 686.6222\n",
      "[epoch 111]  average training loss: 686.5770\n"
     ]
    }
   ],
   "source": [
    "train_adata = sdh.train(5, 0.95, 30, \"./LUNG/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
